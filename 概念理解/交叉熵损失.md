```markdown
# 交叉熵损失（Cross‑Entropy Loss）

交叉熵本质上是衡量 **两个概率分布差异** 的信息量指标，源于信息论。在监督学习中，我们通常比较：

## 两个分布是谁？
- **真实分布 \( \mathbf{y} \)**  
  - 多分类时常是一条 *one‑hot* 向量（某一类为 1，其余为 0）。  
  - 也可能是软标签（标签平滑、知识蒸馏等）。
- **预测分布 \( \hat{\mathbf{y}} \)**  
  - 模型输出的类别概率，一般通过 **softmax**（多分类）或 **sigmoid**（二分类）得到。

---

## 数学定义
- **多分类（\(K\) 类）**  
  \[
    L = -\sum_{i=1}^{K} y_i \,\log \hat{y}_i
    \quad\bigl(\text{若标签为第 } k \text{ 类，则 } L = -\log \hat{y}_k\bigr)
  \]
- **二分类**  
  \[
    L = -\Bigl[\,y\log\hat{y} + (1-y)\log(1-\hat{y})\Bigr],
    \quad y\in\{0,1\},\; \hat{y}\in(0,1)
  \]

---

## 为什么好用
- 与 **softmax/sigmoid** 结合时，梯度形式极简：  
  \[
    \frac{\partial L}{\partial z_i} \;=\; \hat{y}_i - y_i
  \]  
  其中 \( z_i \) 是 logit（做 softmax 之前的分数）。梯度简洁带来 **反向传播高效、数值稳定**。
- 最小化交叉熵等价于最小化 KL 散度 \(D_{\text{KL}}(p\Vert q)\)。当真实分布为 *one‑hot* 时，含义就是 “**把概率尽量压到正确类别上**”。

---

## 实现细节
- 为避免 \(\log 0\) 或 \(\log 1\) 造成数值发散，实际代码会对 \(\hat{y}\) **clip**（如 \(1\text{e-}12\le\hat{y}\le1-1\text{e-}12\)），或直接调用框架自带的 `CrossEntropyLoss` （内部已稳定化）。
- 多分类场景常把 “softmax + 交叉熵” 合并成 **log‑softmax 再负对数** 的实现，进一步减少计算误差。

---

## 直观理解
- 标签告诉模型 **“正确答案”** 在哪里；交叉熵衡量模型给正确答案 **分配了多少注意力**。  
- 若模型把 100% 概率压在正确类别上，损失趋近 0；若概率分散或压错类，损失增大。  

---

> **一句话**：交叉熵损失用信息论的语言告诉模型——  
> **“请尽量少让我惊讶，把概率集中在真实类别上。”**  
> 因此，它是分类任务中最常见、最有效的损失函数之一。
```
